# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_training.ipynb.

# %% auto 0
__all__ = ['VDP', 'Lorenz', 'LotkaVolterra', 'SimODEData', 'create_sim_dataset', 'train']

# %% ../nbs/00_training.ipynb 3
import torch 
import torch.nn as nn
from torchdiffeq import odeint_adjoint as odeint
import pylab as plt
from torch.utils.data import Dataset, DataLoader
from typing import Callable, List, Tuple, Union, Optional

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

print(f"Using the {device} device")

# %% ../nbs/00_training.ipynb 6
class VDP(nn.Module):
    """ 
    Define the Van der Pol oscillator as a PyTorch module.
    """
    def __init__(self, 
                 mu: float, # Stiffness parameter of the VDP oscillator
                 ):
        super().__init__() 
        self.mu = torch.nn.Parameter(torch.tensor(mu))
        
    def forward(self, 
                t: float, # time index
                state: torch.TensorType, # state of the system first dimension is the batch size
                ) -> torch.Tensor:
        x = state[:, 0]
        y = state[:, 1]
        dX = self.mu*(x-1/3*x**3 - y)
        dY = 1/self.mu*x
        dfunc = torch.zeros_like(state)
        dfunc[:, 0] = dX
        dfunc[:, 1] = dY
        return dfunc
    
    def __repr__(self):
        return f" mu: {self.mu.item()}"
    
    

# %% ../nbs/00_training.ipynb 12
class Lorenz(nn.Module):
    """ 
    Define the Lorenz system as a PyTorch module.
    """
    def __init__(self, 
                 sigma: float =10.0, # The sigma parameter of the Lorenz system
                 rho: float=28.0, # The rho parameter of the Lorenz system
                beta: float=8.0/3, # The beta parameter of the Lorenz system
                ):
        super().__init__() 
        self.model_params = torch.nn.Parameter(torch.tensor([sigma, rho, beta]))
        
        
    def forward(self, t, state):
        x = state[:,0]      #variables are part of vector array u 
        y = state[:,1]
        z = state[:,2]
        sol = torch.zeros_like(state)
        
        sigma, rho, beta = self.model_params    #coefficients are part of vector array p
        sol[:,0] = sigma*(y-x)
        sol[:,1] = x*(rho-z) - y
        sol[:,2] = x*y - beta*z
        return sol
    
    def __repr__(self):
        return f" sigma: {self.model_params[0].item()}, rho: {self.model_params[1].item()}, beta: {self.model_params[2].item()}"
    

# %% ../nbs/00_training.ipynb 20
class LotkaVolterra(nn.Module):
    """ 
     The Lotka-Volterra equations are a pair of first-order, non-linear, differential equations
     describing the dynamics of two species interacting in a predator-prey relationship.
    """
    def __init__(self,
                 alpha: float = 1.5, # The alpha parameter of the Lotka-Volterra system
                 beta: float = 1.0, # The beta parameter of the Lotka-Volterra system
                 delta: float = 3.0, # The delta parameter of the Lotka-Volterra system
                 gamma: float = 1.0 # The gamma parameter of the Lotka-Volterra system
                 ) -> None:
        super().__init__()
        self.model_params = torch.nn.Parameter(torch.tensor([alpha, beta, delta, gamma]))
        
        
    def forward(self, t, state):
        x = state[:,0]      #variables are part of vector array u 
        y = state[:,1]
        sol = torch.zeros_like(state)
        
        alpha, beta, delta, gamma = self.model_params    #coefficients are part of vector array p
        sol[:,0] = alpha*x - beta*x*y
        sol[:,1] = -delta*y + gamma*x*y
        return sol
    
    def __repr__(self):
        return f" alpha: {self.model_params[0].item()}, beta: {self.model_params[1].item()}, delta: {self.model_params[2].item()}, gamma: {self.model_params[3].item()}"


# %% ../nbs/00_training.ipynb 25
class SimODEData(Dataset):
    """ 
        A very simple dataset class for simulating ODEs, really could just use
        a tensor of the values directly but for general use it is nice to
        define a Dataset class for your data. This can handle batching and 
        other boiler plate things.
    """
    def __init__(self,
                 ts: List[torch.Tensor], # List of time points
                 values: List[torch.Tensor], # List of dynamical state values at each time point 
                 ) -> None:
        self.ts = ts 
        self.values = values 
        
    def __len__(self) -> int:
        return len(self.ts)
    
    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.ts[index], self.values[index]

      

# %% ../nbs/00_training.ipynb 27
def create_sim_dataset(model: nn.Module, # model to simulate from
                       ts: torch.Tensor, # Time points to simulate at
                       num_samples: int = 10, # Number of samples to generate
                       sigma_noise: float = 0.1, # Noise level to add to the data
                       initial_conditions_default: torch.Tensor = torch.tensor([0.0, 0.0]), # Default initial conditions
                       sigma_initial_conditions: float = 0.1, # Noise level to add to the initial conditions
                       ) -> SimODEData:
    ts_list = [] 
    states_list = [] 
    dim = initial_conditions_default.shape[0]
    for i in range(num_samples):
        x0 = sigma_initial_conditions * torch.randn((1,dim)).detach() + initial_conditions_default
        ys = odeint(model, x0, ts).squeeze(1).detach() 
        ys += sigma_noise*torch.randn_like(ys)
        ts_list.append(ts)
        states_list.append(ys)
    return SimODEData(ts_list, states_list)
    


# %% ../nbs/00_training.ipynb 29
def train(model: torch.nn.Module, # Model to train
          data: SimODEData, # Data to train on
          lr: float = 1e-2, # learning rate for the Adam optimizer
          epochs: int = 10, # Number of epochs to train for
          batch_size: int = 5, # Batch size for training
          ):
    
    trainloader = DataLoader(data, batch_size=batch_size, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.MSELoss()
    
    for epoch in range(epochs):
        running_loss = 0.0 
        for data in trainloader:
            optimizer.zero_grad() # reset gradients
            ts, states = data 
            initial_state = states[:,0,:] # grab the initial state
            pred = odeint(model, initial_state, ts[0]).transpose(0,1)
            loss = criterion(pred, states)
            loss.backward() # compute gradients
            optimizer.step() # update parameters
            running_loss += loss.item() # record loss
        if epoch % 10 == 0:
            print(f"Loss at {epoch}: {running_loss}")


